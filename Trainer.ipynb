{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b5f9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Libraries ===>\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0e23d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Huggin_Face Token ===>\n",
    "login(token=\"your_huggingface_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac69b9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Data Preparation and Preprocessing ===>\n",
    "\n",
    "# === Load Dataset ===\n",
    "dataset = load_dataset('json', data_files=\"your_data_path\")\n",
    "\n",
    "# === Load Tokenizer ===\n",
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# === Set PAD token if missing ===\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# === Apply Chat Format ===\n",
    "def chat_format(examples):\n",
    "    return {\n",
    "        \"prompt\": tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)\n",
    "    }\n",
    "\n",
    "formatted_dataset = dataset.map(chat_format)\n",
    "\n",
    "# === Tokenize Dataset ===\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024\n",
    "    )\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# === Data Collator ===\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# ====== End Data Preparation and Preprocessing ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d397adb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Bits and Bytes Configuration ===>\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217308cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Model Configuration ===>\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9858eaeb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Merge existing LoRA ===>\n",
    "\n",
    "resume_model_path = \"your_model_path_checkpoint\" # if you want to continue your training from the past trained model \n",
    "temp_model = PeftModel.from_pretrained(model, resume_model_path)\n",
    "model = temp_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726a3da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Apply new LoRA configuration ===> # if you don't want to apply new LoRA configuration, comment this block\n",
    "new_lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# === Prepare for stabilities ===>\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b0697",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Apply LoRA to the model ===>\n",
    "\n",
    "# if no new LoRA configuration is needed, replace new_lora_config with resume_model_path\n",
    "model = get_peft_model(model, new_lora_config) \n",
    "model.train()\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47607b10",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Check the Trainable Parameters ===\n",
    "\n",
    "print(f\"Model training mode: {model.training}\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Active adapters: {model.active_adapter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62da40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Training Arguments Configuration ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/Model/chatbot_v3\", # Your save path \n",
    "    per_device_train_batch_size=2,               # GPU memory constraint\n",
    "    gradient_accumulation_steps=16,              #from transformers import TrainingArguments Effective batch size of 16\n",
    "    learning_rate=1.5e-4,                        # Slightly reduced LR for stability\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,                                   # T4 supports fp16\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    optim=\"paged_adamw_8bit\",                    # Better performance on T4\n",
    "    warmup_ratio=0.1,\n",
    "    report_to=None,\n",
    "    gradient_checkpointing=True,\n",
    "    max_grad_norm=0.3,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_first_step=True,\n",
    "    seed=42,\n",
    "    dataloader_pin_memory=True,                  # Re-enable pinning for performance\n",
    "    remove_unused_columns=True,\n",
    "    save_safetensors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c425a9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Trainer ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"=== Final verification ===\")\n",
    "print(f\"Model training mode: {model.training}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Dataset size: {len(tokenized_dataset['train'])}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"Training completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
