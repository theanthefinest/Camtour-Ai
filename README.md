# Camtour-Ai

Comprehensive guide for fine-tuning a Mistral-based causal language model (LoRA + 4-bit quantization) used in this repository.

This README was autogenerated from the project's source files. It explains installation, configuration, training, dataset expectations, and where outputs are saved.

## Table of contents

- Overview
- Project layout
- Requirements
- Quick start (local / Colab)
- Configuration
- Training (run)
- File descriptions
- Troubleshooting
- License & contact

## Overview

This repository provides scripts and utilities to fine-tune a Mistral-family causal LM using:
- Hugging Face Transformers
- BitsAndBytes (4-bit quantization)
- PEFT / LoRA for parameter-efficient fine-tuning

The provided training orchestration uses `transformers.Trainer` and is intended for small-scale experiments or Colab-style runs. Local training on consumer hardware is likely to run out of memory—use cloud GPU or Colab when possible.

## Project layout (key files)

- `README.md` — this file
- `requirements.txt` — Python dependency hints (see below)
- `src/train.py` — main training entrypoint (orchestrates dataset load, model setup, Trainer)
- `src/model_setup.py` — model loading, LoRA configuration, k-bit preparation
- `src/config.py` — model & training configuration (BASE_MODEL, bnb config, TrainingArguments)
- `src/utils.py` — small helpers (Hugging Face login, printing summary)
- `src/chat_format.py` (dataset_prep) — dataset loading and tokenization utilities
- `notebook/Trainer.ipynb` — interactive training notebook (preferred for step-by-step)
- `data/` — expected dataset locations (see Dataset section)

## Requirements

The repository's `requirements.txt` contains the main dependencies. Example (from this repo):

- torch==2.1.2
- transformers==4.36.2
- datasets==2.15.0
- accelerate==0.25.0
- peft==0.7.1
- bitsandbytes==0.41.3
- trl
- scipy
- tensorboardx
- flash-attn

Install in a virtual env (Linux / macOS):

```bash
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

Notes:
- If using CUDA, install the correct torch wheel from PyTorch (match CUDA version).
- `bitsandbytes` sometimes requires a platform-specific wheel. If you see errors, follow the bitsandbytes installation instructions for your platform.

## Quick start

1) Set up configuration:

- Open `src/config.py` and set `HF_TOKEN` to your Hugging Face token (or modify `src/utils.py` to read from environment variables).
- Set `DATA_PATH` to point to your dataset JSON file(s) or a Hugging Face dataset path.
- Set `OUTPUT_DIR` and optionally `RESUME_MODEL_PATH` if you want to resume from a checkpoint.

2) Prepare your data

- This project expects JSON-style datasets consumable by `datasets.load_dataset("json", data_files=DATA_PATH)` as used in `src/chat_format.py`.
- Typical layout:

```
data/
	train-dataset/   # your json files or other dataset files
	test/
```

Each example should match the input format expected by `chat_format` (the dataset prep maps examples to a `prompt` field that will be tokenized).

3) Run training

```bash
# from repo root
python src/train.py
```

`src/train.py` will:
- log in to Hugging Face using `HF_TOKEN` (via `src/utils.py`)
- call the dataset loader/tokenizer in `src/chat_format.py`
- build the model with LoRA (via `src/model_setup.py`)
- run training using `transformers.Trainer` with parameters in `src/config.py`

## Configuration (what to edit)

- `src/config.py` contains these key values:
	- `HF_TOKEN` — your Hugging Face token (required for push/pull)
	- `BASE_MODEL` — e.g., `mistralai/Mistral-7B-Instruct-v0.3`
	- `DATA_PATH` — path/glob to your training JSON files
	- `RESUME_MODEL_PATH` — path to a LoRA checkpoint if resuming
	- `OUTPUT_DIR` — where to save trained models / checkpoints
	- `bnb_config` — BitsAndBytes quantization config
	- `training_args` — a `transformers.TrainingArguments` instance used by `Trainer`

If you want to use environment variables instead of editing files, you can modify `src/utils.py` and `src/config.py` to read `os.environ` values.

## File responsibilities (short)

- `src/train.py` — entrypoint that ties everything together and calls `Trainer.train()`.
- `src/model_setup.py` — loads the base HF model, applies quantization settings, prepares for k-bit training, adds LoRA via PEFT.
- `src/chat_format.py` (or `dataset_prep.py`) — loads dataset with `datasets.load_dataset`, applies a chat formatting template, tokenizes, and returns `tokenized_dataset, data_collator, tokenizer`.
- `src/utils.py` — small utility functions (HF login, basic prints).
- `src/config.py` — place to tune hyperparameters such as batch size, epochs, learning rate, and optimizer settings.

## Outputs

- Checkpoints and final model files are saved under the `OUTPUT_DIR` set in `src/config.py`.
- `Trainer.save_state()` and the `TrainingArguments` `save_strategy` control how often checkpoints are saved.

## Troubleshooting & tips

- Out of memory on GPU: reduce `per_device_train_batch_size` or use gradient accumulation (already configured in training_args).
- bitsandbytes installation: if pip install fails, follow bitsandbytes docs and use a platform-specific wheel.
- Hugging Face auth: ensure `HF_TOKEN` is correct; `src/utils.hf_login()` uses `huggingface_hub.login()`.
- If resuming fails, try clearing `RESUME_MODEL_PATH` or verify it's a PEFT/LoRA checkpoint.
- Use Colab with a T4/P100/V100/A100 for larger experiments. Local consumer GPUs may be insufficient.

## Development notes & next steps

- Add example inference scripts for loading the trained LoRA weights and running generation.
- Add a tiny unit test for `src/chat_format.py` to validate tokenization and prompt formatting.
- Consider making configuration environment-driven to avoid editing tracked files.

## License & Author

Author: LY Chhaythean (Zhang Jiang)

---

If you'd like, I can also:
- add an inference script (example loader + generate)
- make `HF_TOKEN` read from an environment variable and fall back to `src/config.py`
- add a tiny test to validate dataset loading

Tell me which follow-up you'd like and I'll implement it.
